{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronachermann/Amphion/blob/main/assignment_2_q4_rag_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXt6LU7yCthu"
      },
      "source": [
        "# Assignment 2 - RAG\n",
        "\n",
        "Parts which require your interaction are marked with `TODO:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JwUEQnwCuh3"
      },
      "outputs": [],
      "source": [
        "# Install the relevant dependencies\n",
        "!pip3 install datasets sentence_transformers tqdm numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNC0Um-19gtf"
      },
      "source": [
        "Imagine your task is to build a question-answering (QA) system for a company. You are given a language model and have to create this product out of it.\n",
        "The requirements of the system need to adapt very quickly to the new data without training.\n",
        "For this, we will use **Retrieval Augmented Generation (RAG)**.\n",
        "The company insists you use their in-house LM model trained on multiple tasks, a _flan-t5-small_.\n",
        "You can test its QA functionality by asking the question _\"When ETH was founded?\"_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N1qUaDZCthv"
      },
      "outputs": [],
      "source": [
        "# Example inference with the model.\n",
        "# TODO: run me to test the environment\n",
        "\n",
        "from transformers import pipeline\n",
        "vanilla_qa_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=0, truncation=True)\n",
        "\n",
        "QUESTION = \"QUESTION: When was ETH founded?\"\n",
        "\n",
        "vanilla_qa_pipe(f\"{QUESTION} ANSWER:\", max_new_tokens=10)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNIbWYrC9gtf"
      },
      "outputs": [],
      "source": [
        "vanilla_qa_pipe(f\"\"\"\n",
        "        CONTEXT: ETH Zurich (German: Eidgenoessische Technische Hochschule Zurich; English:\n",
        "        Federal Institute of Technology Zurich) is a public research university in Zurich,\n",
        "        Switzerland. Founded in 1854 with the stated mission to educate engineers and scientists,\n",
        "        the university focuses primarily on science, technology, engineering, and mathematics. It\n",
        "        consistently ranks among the top universities in the world and its 16 departments span a\n",
        "        variety of disciplines and subjects.\n",
        "        {QUESTION}\n",
        "        ANSWER:\",\n",
        "    \"\"\",\n",
        "    max_new_tokens=10\n",
        ")[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7E7ivT39gtf"
      },
      "source": [
        "The first output is 1897, which is incorrect.\n",
        "\n",
        "This is not a problem, we can use RAG to automatically provide the passage from an [external source](https://en.wikipedia.org/wiki/ETH_Zurich) and make the model answer. Concatenating the first paragraph from Wikipedia to the question makes the model yield the correct answer 1854."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvfbw-XVCthw"
      },
      "outputs": [],
      "source": [
        "# Define model function; do not modify\n",
        "from typing import List\n",
        "\n",
        "def rag_qa_pipe(question: str, passages: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Define the RAG pipeline which concatenates passages to the question.\n",
        "    :param question: Question text.\n",
        "    :param passages: Relevant text passages.\n",
        "    :return: Generated text from the pipeline.\n",
        "    \"\"\"\n",
        "    passages = \"\\n\".join([f\"CONTEXT: {c}\" for c in passages])\n",
        "    return vanilla_qa_pipe(f\"{passages}\\nQUESTION: {question}\\nANSWER: \", max_new_tokens=10)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKLM2jX59gtf"
      },
      "source": [
        "To make sure you understand the function `rag_qa_pipe`, ask some question without and with some relevant context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJIxcohU9gtg"
      },
      "outputs": [],
      "source": [
        "# TODO: use rag_qa_pipeline some random question that you might have just to test this function\n",
        "\n",
        "print(rag_qa_pipe(\"TODO your question\", []))\n",
        "print(rag_qa_pipe(\"TODO the same question\", [\"TODO add some relevant context, such as from Wikipedia\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMkyWZa9gtg"
      },
      "source": [
        "Start with the provided model and the first 500 questions from the validation part of the _SQuAD_ dataset. The dataset has a ground truth Wikipedia passage linked to it and you can directly use it.\n",
        "\n",
        "Then, compute the QA performance of the model with and without prepended passage using `rag_qa_pipe(question, passages)`.\n",
        "\n",
        "Report the average case-sensitive answer exact match (model output is identical to the gold answer, EM) and case-insensitive [answer F1 scores](https://kierszbaumsamuel.medium.com/f1-score-in-nlp-span-based-qa-task-5b115a5e7d41) (F1) for both setups.\n",
        "Because each question has multiple possible answers, take the maximum score for a model answer across all gold answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boC3-WoQCthx"
      },
      "outputs": [],
      "source": [
        "# baseline model evaluation\n",
        "# TODO: the this cell requires <30 new lines\n",
        "\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"rajpurkar/squad\")\n",
        "\n",
        "def metric_exact_match(ans_pred: str, ans_true: str) -> float:\n",
        "    \"\"\"\n",
        "    Case-sensitive answer exact match, model output is identical to the gold answer.\n",
        "    :param ans_pred: Predicted answer\n",
        "    :param ans_true: Ground truth answer\n",
        "    :return: 1. if the answers are the same, 0. otherwise\n",
        "    \"\"\"\n",
        "    # TODO: ~1 line\n",
        "    return 0.\n",
        "\n",
        "def metric_f1(ans_pred: str, ans_true: str) -> float:\n",
        "    \"\"\"\n",
        "    Case-insensitive answer F1 score.\n",
        "    :param ans_pred: Predicted answer.\n",
        "    :param ans_true: Ground truth answer.\n",
        "    :return: F1 score between the predicted and ground truth answers.\n",
        "    \"\"\"\n",
        "    # TODO: ~10 lines\n",
        "    return 0.\n",
        "\n",
        "for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "    # hint: use `line[\"question\"]`, `line[\"context\"]`, and `line[\"answers\"]`\n",
        "    # TODO: run with and without prepended passage\n",
        "    pass\n",
        "\n",
        "# TODO: Print mean of the exact match and mean of F1 scores for the model with and without prepended passage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x562pF1X9gtg"
      },
      "source": [
        "You will likely see improvements in scores by providing a passage to the model.\n",
        "\n",
        "In contrast to the previous evaluation, during inference in a real world scenario, we do not have access to the ground truth passage.\n",
        "All we have access to is the question from a user.\n",
        "Luckily, the company is providing you with an unstructured knowledge base. This could be the whole of Wikipedia but in our scenario, we use all the passages from the SQuAD dataset and shuffle them to remove any existing structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL7N2sKD9gtg"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "kb = list(set(dataset[\"validation\"][\"context\"]))\n",
        "\n",
        "# make sure that there is no remaining structure\n",
        "random.Random(42).shuffle(kb)\n",
        "print(len(kb), \"passages in the knowledge base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWQi9Z2C9gtg"
      },
      "source": [
        "Now whenever we receive a question, we need to find the relevant passage(s) from the knowledge base and put it in the model input.\n",
        "This is a non-trivial task and a whole research field of Information Retrieval is devoted to it.\n",
        "\n",
        "\n",
        "We are going to convert all the knowledge base passages into vectors using TF-IDF and the provided embedding model ([bert-base-nli-max-tokens](https://huggingface.co/sentence-transformers/bert-base-nli-max-tokens)).\n",
        "The model inference is already implemented for you but you need to fill in all the functions in the `KnowledgeBase` class.\n",
        "You will need to implement the retrieval, the distance metrics, and the three similarity metrics (Euclidean, cosine, inner product).\n",
        "\n",
        "We need to build an abstraction for the knowledge base. It needs to support:\n",
        "- adding new keys (vectors) and their corresponding values\n",
        "- retrieving the closest key given one, based on 3 vector distance metrics\n",
        "\n",
        "The implementation does not need to be efficient.\n",
        "\n",
        "Hint: it's ok to just add all the elements to a list and on retrieval sort the list by the distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFKeERqlCthx"
      },
      "outputs": [],
      "source": [
        "# Knowledge base building. This cell requires <20 new lines.\n",
        "from typing import Literal, List, Any\n",
        "\n",
        "Vec = List\n",
        "Val = Any\n",
        "\n",
        "class KnowledgeBase:\n",
        "    def __init__(self, dim: int):\n",
        "        \"\"\"\n",
        "        Initialize a knowledge base with a given dimensionality.\n",
        "        :param dim: the dimensionality of the vectors to be stored\n",
        "        \"\"\"\n",
        "        # TODO: initialize a persistent structure, such as a simple list\n",
        "        pass\n",
        "\n",
        "    def add_item(self, key: Vec, val: Val):\n",
        "        \"\"\"\n",
        "        Store the key-value pair in the knowledge base.\n",
        "        :param key: key\n",
        "        :param val: value\n",
        "        \"\"\"\n",
        "        # TODO: add to the persistent structure\n",
        "        pass\n",
        "\n",
        "    def retrieve(\n",
        "        self, key: Vec, metric: Literal['l2', 'cos', 'ip'], k: int = 1\n",
        "    ) -> List[Val]:\n",
        "        \"\"\"\n",
        "        Retrieve the top k values from the knowledge base given a key and similarity metric.\n",
        "        :param key: key\n",
        "        :param metric: Similarity metric to use.\n",
        "        :param k: Top k similar items to retrieve.\n",
        "        :return: List of top k similar values.\n",
        "        \"\"\"\n",
        "        # TODO: retrieve the k closest vectors and return their corresponding values\n",
        "        # Hint: this does not have to be efficient, feel free to just sort the whole persistent structure and return the top k\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_euclidean(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute Euclidean (L2) distance between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the Euclidean distance between two vectors\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_cosine(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute the cosine similarity between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the cosine distance between two vectors\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_inner_product(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute the inner product between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the iner product distance between two vectors\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUv2PWPqCthx"
      },
      "outputs": [],
      "source": [
        "# Build knowledge base index\n",
        "# In ideal case this does not need to be changed and can just be run.\n",
        "# Make modifications if you feel they are necessary.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # Sparse retrieval using TF-IDF - vectorize with tfidf and retrieve\n",
        "vectorizer = TfidfVectorizer(max_features=768, norm=None)\n",
        "kb_vectorized = np.asarray(vectorizer.fit_transform([x for x in kb]).todense())\n",
        "kb_index_tfidf = KnowledgeBase(dim=768)\n",
        "for passage_index, passage_embd in enumerate(kb_vectorized):\n",
        "    kb_index_tfidf.add_item(passage_embd.squeeze(), passage_index)\n",
        "\n",
        "# Dense retrieval using Sentence Transformers\n",
        "model_embd = SentenceTransformer(\"bert-base-nli-mean-tokens\").to(\"cuda:0\")\n",
        "kb_index_embd = KnowledgeBase(dim=768)\n",
        "for passage_index, passage_embd in enumerate(tqdm.tqdm(kb)):\n",
        "    kb_index_embd.add_item(model_embd.encode(passage_embd).squeeze(), passage_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlyVH1Zr9gtg"
      },
      "source": [
        "For the same first 500 questions from the validation split evaluate how often is the retrieved passage the correct one (formally Recall@1) or among the top 5 retrieved (Recall@5).\n",
        "Perform the retrieval with three distance metrics: euclidean distance, cosine distance, and inner product. The result for this should be 12 numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzfSY98f9gtg"
      },
      "source": [
        "In production, you receive a question from the user and to answer it, you need to first retrieve the relevant passage(s), pass it to the model, and only then generate the answer.\n",
        "\n",
        "Evaluate the model performance with passages retrieved by TFIDF and EMBD vectorization.\n",
        "Consider top-1 and top-5 passages.\n",
        "This time use only case-insensitive F1.\n",
        "The result for this cell should be |vectorizations $\\times$ passage sizes $\\times$ distance metrics = 2 x 2 x 3 = 12 numbers.\n",
        "\n",
        "Answer the following questions:\n",
        "* Provide one potential advantage and two potential disadvantages of using multiple retrieved passages?\n",
        "* Describe one approach to detect if none of the retrieved passages is relevant to the user question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oazFAFIBP2u2"
      },
      "source": [
        "TODO:\n",
        "* YOUR ANSWER HERE\n",
        "* YOUR ANSWER HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF9KJ8dXCthx"
      },
      "outputs": [],
      "source": [
        "# In ideal case this does not need to be changed and can just be run.\n",
        "# Make modifications if you feel they are necessary.\n",
        "\n",
        "for metric in [\"l2\", \"cos\", \"ip\"]:\n",
        "    print(metric.upper())\n",
        "\n",
        "    for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "        # TODO: evaluate the retrieval\n",
        "        # TODO: store RAG model output\n",
        "        # This requires <30 new lines\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rokgCOi9gtg"
      },
      "source": [
        "Answer the following questions about similarity metrics:\n",
        "* Compare and contrast the three metrics, what they might be influenced by, and their advantages and disadvantages.\n",
        "* Consider the scenario if the vectors in the knowledge base were normalized so that $|x|_2 = 1$. What would the results look like? Hint: look at the formulas with this vector assumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oFY9cn_Qhvr"
      },
      "source": [
        "TODO:\n",
        "\n",
        "* YOUR ANSWER HERE\n",
        "* YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBtKQELq9gtg"
      },
      "source": [
        "Lastly, it is a good practice to analyze failure cases of your solution to better understand the pipeline.\n",
        "Find the first example of each and compute how often the situation happens (percentage). Use the maximum exact match to determine correctness and L2 + embedding for retrieval.\n",
        "\n",
        "- For top-1: The retrieved passage is **correct** but the model is **not correct**.\n",
        "- For top-1: The retrieved passage is **not correct** but the model is **still correct**.\n",
        "- For top-5: One of the retrieved passages is the **correct** one but the model is **not correct**.\n",
        "- For top-1: Without retrieved passage is the model **correct** but with the passage the model becomes **incorrect**.\n",
        "- For top-1: Without retrieved passage is the model **incorrect** and with the passage the model becomes **incorrect** but in a different way (different answer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX0zSnot9gth"
      },
      "outputs": [],
      "source": [
        "# compute the 5 phenomena statistics (relative frequency) and find examples\n",
        "\n",
        "# TODO: <30 lines\n",
        "for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fo6hkfq9gti"
      },
      "source": [
        "A client is complaining that the model answers incorrectly the question _\"Who is the current Governor of Victoria?\"_.\n",
        "1. Show your model output to this question with top-1 retrieved passage using any metric.\n",
        "2. Show which top-1 context is retrieved by L2 embd.\n",
        "\n",
        "Hint for the correct answer, see: [en.wikipedia.org/wiki/Premier_of_Victoria](https://en.wikipedia.org/wiki/Premier_of_Victoria)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTa9yP289gti"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Who is the premier of Victoria?\"\n",
        "# TODO: < 20 lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBnDF4nTOe7"
      },
      "source": [
        "Answer the following questions:\n",
        "* Provide a reason why your model is giving the incorrect answer. (information tracing)\n",
        "* Propose a way by which this could be remedied. (information editing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddscx0DUTTiX"
      },
      "source": [
        "TODO:\n",
        "* YOUR ANSWER HERE\n",
        "* YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KcDRiRl9gti"
      },
      "source": [
        "Note on compute: the GPU time of the gold solution is ~15 minutes. If your solution requires much more compute (e.g. hours), then you are likely doing something incorrectly."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "046b96a8882979e38267a32a551607f1ba06d2ceaf740f4fcb11894ced88ffd3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}